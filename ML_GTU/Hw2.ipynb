{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "###### by Mher Movsisyan\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Conceptual questions  \n",
    "1. Please explain why the first principle component direction (the weight vector) corresponds to the largest eigenvector of the sample covariance matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "It spans the most variance in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the relationship between SVD and eigendecomposition?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "They are both matrix decomposition methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain the three key ideas in ISOMAP (for manifold learning and non-linear dimensionality reduction)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "ISOMAP takes account the geometric structure of the data, and rather than using simple n-dimensional euclidean distance to cluster points it uses nearest neighbors (weighted) to construct a graph (similar to spectral clustering). After constructing the graph it finds the closest path from point to point and approximates the k-dimensional  euclidean distance that way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:  \n",
    "Eigenfaces - done during the interview process, will just review peers works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Order of faces using ISOMAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question aims to reproduce the ISOMAP algorithm results in the original paper for ISOMAP, J.B.\n",
    "Tenenbaum, V. de Silva, and J.C. Langford, Science 290 (2000) 2319-2323 that we have also seen in the\n",
    "lecture as an exercise (isn’t this exciting to go through the process of generating results for a high-impact\n",
    "research paper!)  \n",
    "\n",
    "The file `isomap.mat` (or `isomap.dat`) contains 698 images, corresponding to different poses of the same\n",
    "face. Each image is given as a 64 × 64 luminosity map, hence represented as a vector in $ R^{4096} $. This vector\n",
    "is stored as a row in the file. (This is one of the datasets used in the original paper.) In this question, you\n",
    "are expected to implement the ISOMAP algorithm by coding it up yourself. You may find the shortest path\n",
    "(required by one step of the algorithm), using [this link](https://scikit-learn.org/stable/modules/generated/sklearn.utils.graph_shortest_path.graph_shortest_path.html).  \n",
    "\n",
    "Using Euclidean distance (i.e., in this case, a distance in $ R^{4096} $) to construct the $ \\epsilon $-ISOMAP (follow the\n",
    "instructions in the slides.) You will tune the $ \\epsilon $ parameter to achieve the most reasonable performance. Please\n",
    "note that this is different from K-ISOMAP, where each node has exactly K nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Visualize the nearest neighbor graph (you can either show the adjacency matrix (e.g., as\n",
    "an image), or visualize the graph similar to the lecture slides using graph visualization packages such\n",
    "as Gephi (https://gephi.org) and illustrate a few images corresponds to nodes at different parts of the\n",
    "graph, e.g., mark them by hand or use software packages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implement the ISOMAP algorithm yourself to obtain a two-dimensional low-dimensional\n",
    "embedding. Plot the embeddings using a scatter plot, similar to the plots in lecture slides. Find a few\n",
    "images in the embedding space and show what these images look like and specify the face locations\n",
    "on the scatter plot. Comment on do you see any visual similarity among them and their arrangement,\n",
    "similar to what you seen in the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Now choose ℓ1 distance (or Manhattan distance) between images (recall the definition\n",
    "from “Clustering” lecture)). Repeat the steps above. Use ϵ-ISOMAP to obtain a k = 2 dimensional\n",
    "embedding. Present a plot of this embedding and specify the face locations on the scatter plot. Do\n",
    "you see any difference by choosing a different similarity measure by comparing results in Part (b) and\n",
    "Part (c)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Perform PCA (you can now use your implementation written in Question 1) on the images\n",
    "and project them into the top 2 principal components. Again show them on a scatter plot. Explain\n",
    "whether or you see a more meaningful projection using ISOMAP than PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
