{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "###### by Mher Movsisyan\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:  \n",
    "We want to create a generative binary classification model for classifying nonnegative one-dimensional data. This means, that the labels are binary ($ y \\in \\{0, 1\\} $) and the samples are $ x \\in [0, ∞) $.  \n",
    "We place a uniform prior on $ y $  \n",
    "$$ p(y = 0) = p(y = 1) = \\frac{1}{2} $$  \n",
    "\n",
    "As our samples x are nonnegative, we use exponential distributions (and not Gaussians) as class conditionals:\n",
    "$$ p(x \\vert y = 0) = Expo(x \\vert λ_0) \\text{ and } p(x \\vert y = 1) = Expo(x \\vert λ_1) $$  \n",
    "where $ λ_0 \\neq λ_1 $. Assume, that the parameters $ λ_0 $ and $ λ_1 $ are known and fixed.  \n",
    "a) What is the name of the posterior distribution $ p(y | x) $? You only need to provide the name of the distribution (e.g., “normal”, “gamma”, etc.), not estimate its parameters.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "Gamma distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What values of x are classified as class 1?  \n",
    "(As usual, we assume that the classification decision is $ y_{predicted} = argmax_k p(y = k \\vert x) $)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "For all $ a $ where:  \n",
    "$$ \\int_0^a p(y = k \\vert x) > 0.5 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:  \n",
    "Assume you have a linearly separable data set. What properties does the maximum likelihood solution for the decision boundary $ w $ of a logistic regression model have? Assume that $ w $ includes the bias term.  \n",
    "What is the problem here and how do we prevent it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "Since we have a linearly separable dataset, we have infinitely many hyperplanes that can correctly classify all the data. All those infinite candidates for the decision boundary have equal likelihood on paper, but mose of those would fail to generalize for new data. This is an example of overfitting.  \n",
    "We can introduce regularization or noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3:  \n",
    "Show that the softmax function is equivalent to a sigmoid in the 2-class case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "$$\\text{softmax}(x_1) = \\frac{e^{x_1}}{e^{x_1} + e^{x_2}} \\quad \\text{and} \\quad \\text{softmax}(x_2) = \\frac{e^{x_2}}{e^{x_1} + e^{x_2}}$$\n",
    "\n",
    "Assuming that $x_2 = -x_1$, we can simplify the expression to:\n",
    "\n",
    "$$\\text{softmax}(x_1) = \\frac{e^{x_1}}{e^{x_1} + e^{-x_1}} = \\frac{1}{1 + e^{-2x_1}}$$\n",
    "\n",
    "This is equivalent to the sigmoid function given by:\n",
    "\n",
    "$$\\text{sigmoid}(x_1) = \\frac{1}{1 + e^{-x_1}}$$\n",
    "\n",
    "Therefore, we can see that the softmax function is equivalent to the sigmoid function in the 2-class case when we assume that the second input is the negative of the first input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4:  \n",
    "Which basis function $ \\phi(x_1, x_2) $ makes the data in the example below linearly separable (crosses in one class, circles in the other)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "$ \\phi(x_1, x_2) = x_1 * x_2 $"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
