{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11\n",
    "###### by Mher Movsisyan\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thory Problem 1:\n",
    "\n",
    "Answer:  \n",
    "c) because it is the simmetrical plot the activations of which coincide with the inverse distance between the points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Problem 2:  \n",
    "\n",
    "We train a linear autoencoder to D-dimensional data. The autoencoder has a single K-dimensional hidden layer, there are no biases, and all activation functions are identity (Ïƒ(x) = x).\n",
    "- Why is it usually impossible to get zero reconstruction error in this setting if K < D?\n",
    "\n",
    "Answer:  \n",
    "Because it is impossible to store information that spans D dimensions perfectly in K-dimensional space.\n",
    "\n",
    "\n",
    "- Under which conditions is this possible?\n",
    "\n",
    "Answer:  \n",
    "This is only possible when the information that was in D-dimensions actually only spanned K or lower dimensions (you can think of it as the rank of a matrix)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant recommendation\n",
    "\n",
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not yet rated based on a latent factor model.\n",
    "\n",
    "Specifically, the objective function (loss) we wanted to optimize is:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(u, i) \\in S} (R_{ui} - \\mathbf{q}_u\\mathbf{p}_i^T)^2 + \\lambda\\sum_i{\\left\\lVert \\mathbf{p}_i  \\right\\rVert}^2 + \\lambda\\sum_u {\\left\\lVert\\mathbf{q}_u  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $S$ is the set of $(u, i)$ pairs for which the rating $R_{ui}$ given by user $u$ to restaurant $i$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda$ is hyper-parameter that control the strength of the regularization.\n",
    "\n",
    "The task it to solve the matrix factorization via alternating least squares _and_ stochastic gradient descent (non-batched, you may omit the bias).\n",
    "\n",
    "**Hint 1**: Using the closed form solution for regression might lead to singular values. To avoid this issue perform the regression step with an existing package such as scikit-learn. It is advisable to use ridge regression to account for regularization.\n",
    "\n",
    "**Hint 2**: If you are using the scikit-learn package remember to set ``fit_intercept = False`` to only learn the coefficients of the linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data (nothing to do here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"hw_11_matrix_factorization_ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset. We store the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.max(ratings[:,0] + 1)\n",
    "n_restaurants = np.max(ratings[:,1] + 1)\n",
    "R = sp.coo_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), shape=(n_users, n_restaurants)).tocsr()\n",
    "R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"> cold start problem</a>, in the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings.\n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.\n",
    "\n",
    "**Note**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. We store the indices for which we the rating data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        nnz = matrix > 0\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        matrix = matrix[row_ixs]\n",
    "        nnz = matrix > 0\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        matrix = matrix[:, col_ixs]\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a function that subtracts the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute the modified matrix and user_means\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    user_means = np.asarray(matrix.sum(axis=1) / (matrix!=0).sum(axis=1))\n",
    "    matrix = matrix - np.diag(user_means.flatten())\n",
    "    ## END SOLUTION\n",
    "    \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means.reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train, validation and test set (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_cp = matrix.copy()\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "    val_idx = tuple(ixs[:n_validation].T)\n",
    "    test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "    \n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "    \n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "R = cold_start_preprocessing(R, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "R_train, val_idx, test_idx, val_values, test_values = split_data(R, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3529,3529)  and requested shape (3529,2072)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Remove user means.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nonzero_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margwhere(R_train)\n\u001b[1;32m----> 3\u001b[0m R_shifted, user_means \u001b[39m=\u001b[39m shift_user_mean(R_train)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Apply the same shift to the validation and test data.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m val_values_shifted \u001b[39m=\u001b[39m val_values \u001b[39m-\u001b[39m user_means[np\u001b[39m.\u001b[39marray(val_idx)\u001b[39m.\u001b[39mT[:,\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mA1\n",
      "Cell \u001b[1;32mIn[34], line 23\u001b[0m, in \u001b[0;36mshift_user_mean\u001b[1;34m(matrix)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# TODO: Compute the modified matrix and user_means\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m## BEGIN SOLUTION\u001b[39;00m\n\u001b[0;32m     22\u001b[0m user_means \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(matrix\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m (matrix\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m matrix \u001b[39m=\u001b[39m matrix \u001b[39m-\u001b[39;49m np\u001b[39m.\u001b[39;49mdiag(user_means\u001b[39m.\u001b[39;49mflatten())\n\u001b[0;32m     24\u001b[0m \u001b[39m## END SOLUTION\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(np\u001b[39m.\u001b[39misclose(matrix\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m), \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\sparse\\_base.py:493\u001b[0m, in \u001b[0;36mspmatrix.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sub_sparse(other)\n\u001b[0;32m    492\u001b[0m \u001b[39melif\u001b[39;00m isdense(other):\n\u001b[1;32m--> 493\u001b[0m     other \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mbroadcast_to(other, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sub_dense(other)\n\u001b[0;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_to\u001b[39m(array, shape, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    369\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39m           [1, 2, 3]])\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m _broadcast_to(array, shape, subok\u001b[39m=\u001b[39;49msubok, readonly\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall elements of broadcast shape must be non-\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    347\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    348\u001b[0m extras \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 349\u001b[0m it \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnditer(\n\u001b[0;32m    350\u001b[0m     (array,), flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmulti_index\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrefs_ok\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mzerosize_ok\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m extras,\n\u001b[0;32m    351\u001b[0m     op_flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mreadonly\u001b[39;49m\u001b[39m'\u001b[39;49m], itershape\u001b[39m=\u001b[39;49mshape, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m it:\n\u001b[0;32m    353\u001b[0m     \u001b[39m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     broadcast \u001b[39m=\u001b[39m it\u001b[39m.\u001b[39mitviews[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3529,3529)  and requested shape (3529,2072)"
     ]
    }
   ],
   "source": [
    "# Remove user means.\n",
    "nonzero_indices = np.argwhere(R_train)\n",
    "R_shifted, user_means = shift_user_mean(R_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[np.array(val_idx).T[:,0]].A1\n",
    "test_values_shifted = test_values - user_means[np.array(test_idx).T[:,0]].A1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the loss function (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs).\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.array, shape [n_ixs,]\n",
    "        The array with the ground-truth values.\n",
    "    ixs : tuple, shape [2, n_ixs]\n",
    "        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).\n",
    "    Q : np.array, shape [N, k]\n",
    "        The matrix Q of a latent factor model.\n",
    "    P : np.array, shape [k, D]\n",
    "        The matrix P of a latent factor model.\n",
    "    reg_lambda : float\n",
    "        The regularization strength\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "           The loss of the latent factor model.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    regularization_loss =  reg_lambda * (np.sum(np.linalg.norm(P, axis=0)**2) + np.sum(np.linalg.norm(Q, axis=1) ** 2))\n",
    "    \n",
    "    return mean_sse_loss + regularization_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating optimization\n",
    "\n",
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement a function that initializes the latent factors $Q$ and $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, \n",
    "             'random' means we initialize the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # TODO: Compute Q and P\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "\n",
    "    ## END SOLUTION\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the alternating optimization approach and stochastic gradient approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(R, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=5, eval_every=1, optimizer='sgd', lr=1e-2):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    R                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "                        \n",
    "    optimizer         : str, optional, default: sgd\n",
    "                        If `sgd` stochastic gradient descent shall be used. Otherwise, use alternating least squares.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute best_Q, best_P, validation_losses, train_losses and converged_after\n",
    " \n",
    "    ## BEGIN SOLUTION\n",
    "   \n",
    "    ## END SOLUTION\n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the latent factor (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_sgd, P_sgd, val_loss_sgd, train_loss_sgd, converged_sgd = latent_factor_alternating_optimization(\n",
    "    R_shifted, nonzero_indices, k=100, val_idx=val_idx, val_values=val_values_shifted, \n",
    "    reg_lambda=1e-4, init='random', max_steps=100, patience=10, optimizer='sgd', lr=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_als, P_als, val_loss_als, train_loss_als, converged_als = latent_factor_alternating_optimization(\n",
    "    R_shifted, nonzero_indices, k=100, val_idx=val_idx, val_values=val_values_shifted, \n",
    "    reg_lambda=1e-4, init='random', max_steps=100, patience=10, optimizer='als'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the validation and training losses over for each iteration (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[10, 5])\n",
    "fig.suptitle(\"Alternating optimization, k=100\")\n",
    "\n",
    "ax[0].plot(train_loss_sgd[1::], label='sgd')\n",
    "ax[0].plot(train_loss_als[1::], label='als')\n",
    "ax[0].set_title('Training loss')\n",
    "ax[0].set_xlabel(\"Training iteration\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(val_loss_sgd[1::], label='sgd')\n",
    "ax[1].plot(val_loss_als[1::], label='als')\n",
    "ax[1].set_title('Validation loss')\n",
    "ax[1].set_xlabel(\"Training iteration\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder and t-SNE (OPTIONAL)\n",
    "\n",
    "Hereinafter, we will implement an autoencoder and analyze its latent space via interpolations and t-SNE. For this, we will use the famous Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** If you run into memory issues simply reduce the `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionMNIST(root='data', download=True, train=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = FashionMNIST(root='data', download=True, train=False, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True, \n",
    "                                           num_workers=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False, \n",
    "                                          num_workers=2, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Define decoder network\n",
    "\n",
    "Feel free to choose any architecture you like. Our model was this:\n",
    "```\n",
    "Autoencoder(\n",
    "  (encode): Sequential(\n",
    "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (1): LeakyReLU(negative_slope=0.01)\n",
    "    (2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (4): LeakyReLU(negative_slope=0.01)\n",
    "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): LeakyReLU(negative_slope=0.01)\n",
    "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (9): LeakyReLU(negative_slope=0.01)\n",
    "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (11): LeakyReLU(negative_slope=0.01)\n",
    "  )\n",
    "  (decode): Sequential(\n",
    "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (1): LeakyReLU(negative_slope=0.01)\n",
    "    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
    "    (3): LeakyReLU(negative_slope=0.01)\n",
    "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (5): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
    "    (6): LeakyReLU(negative_slope=0.01)\n",
    "    (7): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (8): ConvTranspose2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (9): ConvTranspose2d(4, 1, kernel_size=(3, 3), stride=(1, 1))\n",
    "    (10): Sigmoid()\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    ## BEGIN SOLUTION\n",
    "\n",
    "    ## END SOLUTION\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_approx = self.decode(z)\n",
    "        \n",
    "        assert x.shape == x_approx.shape\n",
    "        return x_approx\n",
    "    \n",
    "print(Autoencoder())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model transform the image from $28 \\cdot 28 = 784$ dimensional space down into a $32 \\cdot 3 \\cdot 3 = 288$ dimensional space. However, note that the latent space also must contain some spatial information that the decoder needs for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_dataset[0][0][None, ...]\n",
    "z = Autoencoder().encode(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(z.shape)\n",
    "print(Autoencoder().decode(z).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Train the autoencoder\n",
    "\n",
    "Of course, we must train the autoencoder if we want to analyze it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "log_every_batch = 20\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss_trace = []\n",
    "    for batch, (x, _) in enumerate(train_loader):\n",
    "        # TODO: The autoendocer shall be trained on the mse loss\n",
    "        ## BEGIN SOLUTION\n",
    "\n",
    "        ## END SOLUTION\n",
    "        train_loss_trace.append(loss.detach().item())\n",
    "        if batch % log_every_batch == 0:\n",
    "            print(f'Training: Epoch {epoch} batch {batch} - loss {loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss_trace = []\n",
    "    for batch, (x, _) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        x_approx = model(x)\n",
    "        loss = F.mse_loss(x_approx, x)\n",
    "        test_loss_trace.append(loss.detach().item())\n",
    "        if batch % log_every_batch == 0:\n",
    "            print(f'Test: Epoch {epoch} batch {batch} loss {loss}')\n",
    "    print(f'Epoch {epoch} finished - average train loss {np.mean(train_loss_trace)}, '\n",
    "          f'average test loss {np.mean(test_loss_trace)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent = []\n",
    "    for batch, (x, _) in enumerate(test_loader):\n",
    "        latent.append(model.encode(x.to(device)).cpu())\n",
    "    latent = torch.cat(latent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and t-SNE (nothing to do here)\n",
    "Next, we are going to look at some random images and their embeddings. Since 7x7 is still too large to visialize further dimensionality reduction techniques are required. \n",
    "\n",
    "It is not uncommand that a neural network designer wants to understand whats going on in the latent space and therefore uses techniques such as t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(test_dataset: torch.utils.data.Dataset, z_test: torch.Tensor, count: int, \n",
    "                technique: str, perplexity: float = 30):\n",
    "    \"\"\"\n",
    "    Fit t-SNE or PCA and plots the latent space. Moreover, we then display the correspeondig image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_dataset  : torch.utils.data.DataSet\n",
    "                    Dataset containing raw images to display.\n",
    "    z_test        : torch.Tensor\n",
    "                    The transformed images.\n",
    "    count         : int\n",
    "                    Number of random images to sample\n",
    "    technique     : str\n",
    "                    Either \"pca\" or \"tsne\". Otherwise, a ValueError is thrown.\n",
    "    perplexity    : float, optional, default: 30.0\n",
    "                    Perplexity is t-SNE is used.\n",
    "        \n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(z_test), count, replace=False)\n",
    "    inputs = z_test[indices]\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.set_title(technique)\n",
    "    if technique == 'pca':\n",
    "        coords = PCA(n_components=2).fit_transform(inputs.reshape(count, -1))\n",
    "    elif technique == 'tsne':\n",
    "        coords = TSNE(n_components=2, perplexity=perplexity).fit_transform(inputs.reshape(count, -1))\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    for idx, (x, y) in zip(indices, coords):\n",
    "        im = OffsetImage(test_dataset[idx][0].squeeze().numpy(), zoom=1, cmap='gray')\n",
    "        ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    ax.update_datalim(coords)\n",
    "    ax.autoscale()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 1000, 'pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 300, 'tsne', perplexity=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 300, 'tsne', perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 300, 'tsne', perplexity=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 300, 'tsne', perplexity=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(test_dataset, latent, 300, 'tsne', perplexity=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Linear Interpolation on the latent space\n",
    "If the latent space has learned something meanigfull, we can leverage this for further analysis/downstream tasks. Anyways, we were wondering all along how the interpolation between a shoe and a pullover might look like.\n",
    "\n",
    "For this we encode two images $z_i = f_{enc}(x_i)$ and $z_j = f_{enc}(x_j)$. Then we linearily interpolate $k$ equidistant locations on the line between $z_i$ and $z_j$. Those locations are then be decoded by the decoder network $f_{dec}(\\dots)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def interpolate_between(model: Autoencoder, test_dataset: torch.utils.data.Dataset, idx_i: int, idx_j: int, n = 12):\n",
    "    \"\"\"\n",
    "    Plot original images and the reconstruction of the linear interpolation in the respective latent space embedding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model         : Autoencoder\n",
    "                    The (trained) autoencoder.\n",
    "    test_dataset  : torch.utils.data.Dataset\n",
    "                    Test images.\n",
    "    idx_i         : int\n",
    "                    Id for first image.\n",
    "    idx_j         : int\n",
    "                    Id for second image.\n",
    "    n             : n, optional, default: 1\n",
    "                    Number of intermediate interoplations (including original reconstructions).\n",
    "        \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=[6, 4])\n",
    "    fig.suptitle(\"Original images\")\n",
    "    ax[0].imshow(test_dataset[idx_i][0][0].numpy(), cmap='gray')\n",
    "    ax[1].imshow(test_dataset[idx_j][0][0].numpy(), cmap='gray')\n",
    "    \n",
    "    # Get embedding\n",
    "    z_i = model.encode(test_dataset[idx_i][0].to(device)[None, ...])[0]\n",
    "    z_j = model.encode(test_dataset[idx_j][0].to(device)[None, ...])[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(2, n//2, figsize=[15, 8])\n",
    "    ax = [sub for row in ax for sub in row]\n",
    "    fig.suptitle(\"Reconstruction after interpolation in latent space\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # TODO: Linearily interpolate between `z_i` and `z_j` in `n` equidistant steps. \n",
    "        # Then decode the embedding and plot the image and add the percentage as a title.\n",
    "        ## BEGIN SOLUTION\n",
    "\n",
    "        ## END SOLUTION\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_between(model, test_dataset, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_between(model, test_dataset, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_between(model, test_dataset, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
